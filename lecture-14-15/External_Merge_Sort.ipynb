{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import io_backend as io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: this notebook requires `io_backend.py` and `compModel.js` to be in the same directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Merge Sort\n",
    "------------------\n",
    "\n",
    "Sorting big data is easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 690 ms, sys: 10.7 ms, total: 701 ms\n",
      "Wall time: 840 ms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "x = range(1000000); random.shuffle(x)\n",
    "%time x.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except that sorting a million integers isn't cool... you know what's cool?  Sorting a billion!  Try it!\n",
    "\n",
    "Just kidding! (Hint: if you took the above seriously, select \"Kernel >> Restart\" in the notebook menu bar and then select \"Clear all outputs & restart\" to save your computer)\n",
    "\n",
    "What actually happens when someone- or more pertinently, some DBMS- wants to sort data that is bigger than main memory (e.g. RAM)?  For example, the rows of a very large table in a database?\n",
    "\n",
    "In this notebook we'll look at one classic algorithm for doing this efficently: **external merge sort**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: External Merge\n",
    "---------------------\n",
    "\n",
    "First, we'll look at a simple way to efficiently merge a file larger than memory.  Suppose we have the following scenario:\n",
    "* _Input:_ Two sorted lists of length $N$ and $M$\n",
    "* _Output:_ One sorted list of length $N+M$\n",
    "\n",
    "We will see a way to do this that requires at least 3 buffer pages, and is $O(2*(M+N))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def min_non_null_idx(a):\n",
    "    arr = filter(lambda x : x[1] is not None, enumerate(a))\n",
    "    return min(arr, key=lambda x : x[1])\n",
    "\n",
    "def external_merge(b, file_in_ids, erase=False):\n",
    "    P = b.page_size\n",
    "    if len(file_in_ids) + 1 > b.buffer_size:\n",
    "        raise Exception(\"Too many files for buffer size.\")\n",
    "\n",
    "    # A FileIterator will iterate through elements of a file's pages\n",
    "    # reading in & releasing pages so as to take up one page in buffer\n",
    "    fis = [io.FileIterator(b, fid) for fid in file_in_ids]\n",
    "    elements = [fi.get_next() for fi in fis]\n",
    "\n",
    "    # A FileWriter object will append elements to a file, creating\n",
    "    # & flushing pages so as to take up one page in buffer\n",
    "    fid_out = b.new_file()\n",
    "    fw = io.FileWriter(b, fid_out)\n",
    "    \n",
    "    # Successively choose the smallest of the B files' smallest elements\n",
    "    while any([e is not None for e in elements]):\n",
    "        \n",
    "        # Erase & then output the smallest element from the pages currently in buffer\n",
    "        min_i, min_e = min_non_null_idx(elements)\n",
    "        if erase:\n",
    "            fis[min_i].erase_current()\n",
    "        fw.append(min_e)\n",
    "        \n",
    "        # Get the next element from the file we just picked one from\n",
    "        elements[min_i] = fis[min_i].get_next()\n",
    "    fw.close()\n",
    "    return fid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_rand_file(b, l, sorted=False):\n",
    "    vals = random.sample(range(10*l), l)\n",
    "    if sorted:\n",
    "        vals.sort()\n",
    "    fid = b.new_file()\n",
    "    fw = io.FileWriter(b, fid)\n",
    "    for v in vals:\n",
    "        fw.append(v)\n",
    "    fw.close()\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the buffer + two random sorted files\n",
    "N = 4\n",
    "M = 5\n",
    "b = io.Buffer(page_size=2, buffer_size=3)\n",
    "fids = [\n",
    "    new_rand_file(b, N, sorted=True),\n",
    "    new_rand_file(b, M, sorted=True)\n",
    "]\n",
    "b.display_set_mark()  # Don't animate this setup part\n",
    "\n",
    "# Merge!\n",
    "merged_fid = external_merge(b, fids, erase=True)\n",
    "\n",
    "# NOTE THAT YOU CAN ADJUST THE ANIMATION SPEED!!!\n",
    "b.display(speed=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for two files and three buffer pages, this algorithm is\n",
    "\n",
    "$O(2*(M+N))$,\n",
    "\n",
    "since for each page read in from disk ($M+N$ total), there is a page flushed out.  Furthermore, since the algorithm above is fully generalizable to $K$ files and a buffer of size $K+1$ or greater, we see that if file $i$ has $N_i$ pages, then we again are\n",
    "\n",
    "$O\\left(2*\\sum_{i=1}^KN_i\\right)$\n",
    "\n",
    "which is just the above in more general form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External Merge Sort\n",
    "------------------\n",
    "\n",
    "Now that we can merge two sorted files, sorting is easy: we just\n",
    "\n",
    "1. Split our data up into files small enough to _sort in memory_ (i.e. small enough to fit in the buffer)\n",
    "2. Repeatedly _merge these files_ with the above algorithm until they are again one (now sorted) file!\n",
    "\n",
    "Since space is an issue for the visualizations, we'll just show a simple sort-merge with one round of merging below, however the algorithm is fully extendable to larger files with more merges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for (1) splitting and sorting in-memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def flatten(x): return list(itertools.chain.from_iterable(x))\n",
    "\n",
    "def split_and_sort_in_mem(b, fid_in):\n",
    "    fids_out = []\n",
    "    N = b.get_file_len(fid_in)\n",
    "    B = b.buffer_size\n",
    "    P = b.page_size\n",
    "    for i in range(0, N, B):\n",
    "        fid_out = b.new_file()\n",
    "        \n",
    "        # Read in enough pages to fill the buffer if possible\n",
    "        pages = [b.read(fid_in, pid) for pid in range(i, min(i+B, N))]\n",
    "        \n",
    "        # Collect & sort the data from the pages in memory\n",
    "        vals = sorted(flatten([page.get_data_copy() for page in pages]))\n",
    "        \n",
    "        # Write out to the new file\n",
    "        for j,page in enumerate(pages):\n",
    "            b.release(page)\n",
    "            page = b.new_page(fid_out)\n",
    "            page.set_all(vals[j*P:(j+1)*P])\n",
    "            b.flush(page)\n",
    "        fids_out.append(fid_out)\n",
    "    \n",
    "    # Delete original file & return new file ids\n",
    "    b.delete_file(fid_in)\n",
    "    return fids_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for (2) recursively applying external merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursive_external_merge(b, fids, erase=False):\n",
    "    L = len(fids)\n",
    "    if L < b.buffer_size:\n",
    "        return external_merge(b, fids, erase=erase)\n",
    "    else:\n",
    "        mid_1 = recursive_external_merge(b, fids[:(L/2)], erase=erase)\n",
    "        mid_2 = recursive_external_merge(b, fids[(L/2):], erase=erase)\n",
    "        return external_merge(b, [mid_1, mid_2], erase=erase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The full **external merge-sort** algorithm:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def external_merge_sort(b, fid, erase=False, concise=False):\n",
    "    \n",
    "    # Split into size-B chunks, sort in memory, save to new files\n",
    "    fids = split_and_sort_in_mem(b, fid)\n",
    "    \n",
    "    # Optionally set mark here so as not to animate the previous two steps\n",
    "    if concise:\n",
    "        b.display_set_mark()\n",
    "        \n",
    "    # Merge the sorted files recursively\n",
    "    return recursive_external_merge(b, fids, erase=erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create buffer & random file larger than buffer\n",
    "b = io.Buffer(buffer_size=3, page_size=2)\n",
    "fid = new_rand_file(b, 11)\n",
    "b.display_set_mark()\n",
    "\n",
    "# Use the algorithm\n",
    "external_merge_sort(b, fid, erase=True, concise=True)\n",
    "b.display(buffer_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the cost of this algorithm?\n",
    "\n",
    "Initial part (splitting and in-memory sorting): $2*N$\n",
    "\n",
    "Recursive external merge: $\\text{ceil}\\left(\\log_B\\left(\\frac{N}{B+1}\\right)\\right)$ passes, each involving $2*N$ IO operations\n",
    "\n",
    "$\\implies O\\left(2N*\\left(\\text{ceil}\\left(\\log_B\\left(\\frac{N}{B+1}\\right)\\right) + 1\\right)\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
